{"cells":[{"cell_type":"markdown","metadata":{"id":"6xO-XBsYMOiR"},"source":["#Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37175,"status":"ok","timestamp":1699264206201,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"BHkSCcGWfkls","outputId":"bd2ffc58-6ee3-4cd5-e158-f55b46013ec1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["use_gdrive = True # @param {type:\"boolean\"}\n","\n","RESULTS_PATH = \"results1\"\n","\n","if use_gdrive:\n","  try:\n","    # mount your google drive to get permanent storage for your results\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    RESULTS_PATH = \"/content/drive/MyDrive/infoseclab23/results1\"\n","  except ModuleNotFoundError:\n","    print(\"failed to mount gdrive\")\n","else:\n","  print(f\"saving results to '{RESULTS_PATH}'. If you're using Google Colab, this folder will be deleted when you disconnect!\")\n","\n","!mkdir -p {RESULTS_PATH}"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9591,"status":"ok","timestamp":1699264220529,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"aCjvBq6rL0n1","outputId":"1b15048b-30a2-4672-d504-da1ab34b1773"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'infoseclab_23'...\n","remote: Enumerating objects: 34, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 34 (delta 0), reused 2 (delta 0), pack-reused 30\u001b[K\n","Receiving objects: 100% (34/34), 268.09 MiB | 38.93 MiB/s, done.\n","Resolving deltas: 100% (1/1), done.\n","/content/infoseclab_23\n","From https://github.com/ethz-spylab/infoseclab_23\n"," * branch            HEAD       -> FETCH_HEAD\n","Already up to date.\n","/content\n"]}],"source":["import sys\n","\n","# Lab files\n","![ ! -d 'infoseclab_23' ] && git clone https://github.com/ethz-spylab/infoseclab_23.git\n","%cd infoseclab_23\n","!git pull https://github.com/ethz-spylab/infoseclab_23.git\n","%cd ..\n","if \"infoseclab_23\" not in sys.path:\n","  sys.path.append(\"infoseclab_23\")"]},{"cell_type":"markdown","metadata":{"id":"LXC5q0RvNhhh"},"source":["#Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9258,"status":"ok","timestamp":1699264232243,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"Uwi_QoU9Nguf"},"outputs":[],"source":["import torch\n","import numpy as np\n","import infoseclab\n","from infoseclab import defenses, ImageNet, EPSILON, utils, evaluation\n","import os\n","from zipfile import ZipFile\n","\n","device = \"cuda\""]},{"cell_type":"markdown","metadata":{"id":"KHhM612yjnBu"},"source":["# 0.&nbsp; A quick primer on constrained optimization in PyTorch\n","\n","To get a feel for how to optimize functions in PyTorch over a domain, below we solve a simple 1-dimensional function mimization problem.\n","\n","We want to find the minimum of $f(x)$ under the constraint $x \\in [-1, 1]$.\n","\n","(the actual minimimum is at $x=\\sqrt{2/3} \\approx 0.8165$ and has value $f(x) \\approx -6.089$)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1418,"status":"ok","timestamp":1699255852449,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"tIsix6s_jmbN","outputId":"be4111b4-b14e-424a-cab6-b825ed4f78fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: f(0.00) = -5.00\n","step 1: f(0.05) = -5.10\n","step 2: f(0.10) = -5.20\n","step 3: f(0.15) = -5.30\n","step 4: f(0.20) = -5.39\n","step 5: f(0.25) = -5.48\n","step 6: f(0.30) = -5.57\n","step 7: f(0.35) = -5.66\n","step 8: f(0.40) = -5.74\n","step 9: f(0.45) = -5.81\n","step 10: f(0.50) = -5.88\n","step 11: f(0.55) = -5.93\n","step 12: f(0.60) = -5.98\n","step 13: f(0.65) = -6.03\n","step 14: f(0.70) = -6.06\n","step 15: f(0.75) = -6.08\n","step 16: f(0.80) = -6.09\n","step 17: f(0.85) = -6.09\n","step 18: f(0.80) = -6.09\n","step 19: f(0.85) = -6.09\n"]}],"source":["# the function we want to minimize\n","def f(x):\n","  return x**3 - 2*x - 5\n","\n","# our starting point\n","x = torch.zeros(1).to(device)\n","\n","for i in range(20):\n","  x = x.requires_grad_(True)  # we want to take gradients with respect to x\n","  objective = f(x)            # compute the current objective\n","  objective.backward()        # take the gradient of the objective with respect to all inputs\n","  grad = x.grad.detach()      # get the value of the gradient with respect to x\n","\n","  print(f\"step {i}: f({x.item():.2f}) = {objective.item():.2f}\")\n","\n","  with torch.no_grad():\n","    x = x - 0.05 * torch.sign(grad)  # take a gradient update step to minimize the objective\n","    x = torch.clamp(x, -1, 1)        # ensure we stay in the allowed range"]},{"cell_type":"markdown","metadata":{"id":"HV7wnlsjNyYb"},"source":["# 1.&nbsp;Targeted PGD attack on undefended ResNet-50\n","\n","We will first run a simple *targeted* PGD attack, where the goal is to get the model to misclassify an input `(x, y)` into a specific incorrect class `y'`.\n","\n","You can design your attack however you'd like, but we recommend first\n","implementing a `project` method that projects an adversarial example\n","onto the Lp ball centered at the original sample. It is customary to\n","also ensure that the projected sample is in the valid range for an image (i.e.,\n","all pixel values in [0, 255]).\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699264232243,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"bEsJ9nz_N11y"},"outputs":[],"source":["class PGD(object):\n","    \"\"\"\n","    A targeted PGD attack in l_inf norm.\n","    \"\"\"\n","    def __init__(self, epsilon, clf):\n","        \"\"\"\n","        :param epsilon: the maximum perturbation allowed\n","        :param clf: the classifier to attack\n","        \"\"\"\n","        self.epsilon = epsilon\n","        self.clf = clf\n","\n","    def project(self, x_adv, x_orig):\n","        \"\"\"\n","        Project x_adv onto the epsilon ball around x_orig.\n","        :param x_adv: the adversarial images\n","        :param x_orig: the clean images\n","        :return: the adversarial images projected onto the epsilon ball, in the range [0, 255]\n","        \"\"\"\n","        # the norm of the noise should be less than eps\n","        eta = torch.clamp(x_adv - x_orig, min=-self.epsilon, max=self.epsilon)\n","        # then we project to the closes value in the tollerance ball\n","        return torch.clamp(x_orig + eta, min=0, max=255).detach_()\n","\n","\n","\n","    def attack_batch(self, x, y_targets):\n","        \"\"\"\n","        Attack a batch of images with PGD.\n","        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","        :param y_targets: the target labels of size (batch_size,)\n","        :return: the adversarial images of size (batch_size, 3, 224, 224)\n","        \"\"\"\n","        loss = torch.nn.CrossEntropyLoss()\n","        x_orig = x.clone()\n","        alpha = 0.2\n","\n","        for i in range(30):\n","          x.requires_grad = True\n","          outputs = self.clf.get_logits(x)\n","\n","          cost = loss(outputs, y_targets)\n","          cost.backward()\n","\n","          with torch.no_grad():\n","            x_adv = x - alpha * x.grad.sign()\n","            x = self.project(x_adv, x_orig)\n","\n","        return x\n","\n","\n","    def attack_all(self, images, labels, batch_size=20):\n","        \"\"\"\n","        A utility to attack all images in the dataset by batching.\n","        :param images: the images to attack, of size (N, 3, 224, 224) in the range [0, 255]\n","        :param labels: the target labels\n","        :param batch_size: the batch size to use\n","        :return: the adversarial images\n","        \"\"\"\n","        return utils.batched_func(self.attack_batch, inputs=(images, labels),\n","                                  batch_size=batch_size,\n","                                  device=self.clf.device)\n","\n","# load the defense\n","resnet = defenses.ResNet(device)\n","\n","pgd = PGD(epsilon=EPSILON, clf=resnet)\n","x_adv = pgd.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n","\n","utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), x_adv)\n","evaluation.eval_targeted_pgd(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"executionInfo":{"elapsed":232,"status":"error","timestamp":1699131889993,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"V0J7Anb0ani3","outputId":"69c2118a-9182-4c90-e6e3-0f738bfb677c"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-de126f9346f0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# you can visualize your attack samples as follows if this helps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_orig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImageNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_adv' is not defined"]}],"source":["# you can visualize your attack samples as follows if this helps\n","idx = 0\n","input = torch.stack([x_adv[idx], ImageNet.clean_images[idx]]).to(device)\n","logits = resnet.get_logits(input)\n","utils.display(x_adv[idx], image_orig=ImageNet.clean_images[idx], logits=logits)"]},{"cell_type":"markdown","metadata":{"id":"m9myeLu4a5xB"},"source":["# 2.&nbsp;Evading Detection\n","\n","It turns out that \"naive\" adversarial examples are very easy to *detect*.\n","So one could build a defense that aims to detect when an input has been perturbed, to reject it and raise an alarm.\n","\n","Unfortunately, as we'll see such defenses are hard to make robust against an *adaptive* attacker that also optimizes over the detector.\n","\n","You will now implement attacks against two detector defenses:\n","\n","<ul>\n","  <li> 2.1. A detector using a standard neural network. </li>\n","  <li> 2.2. A Random Forest detector </li>\n","<ul>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4705,"status":"ok","timestamp":1698936507937,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"wEsVJ2M3a5Ku","outputId":"1d413338-7fa7-48b2-f2c1-c60731a94a48"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== Evaluating targeted PGD with Neural Network Detector ===\n","\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.0% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 100.0% (target: ≥98.0%)\u001b[0m\n","\u001b[32m\tclean examples detected: 2.5% (target: ≤5.0%)\u001b[0m\n","\u001b[31m\tadv examples detected: 84.5% (target: ≤1.0%)\u001b[0m\n","\u001b[31mNOT THERE YET!\u001b[0m\n"]}],"source":["# Your previous attack is likely easily detected\n","evaluation.eval_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device=device);"]},{"cell_type":"markdown","metadata":{"id":"cpHESdu-h6bO"},"source":["## 2.1&nbsp;Evading A Neural Network Detector\n","\n","We will first do a targeted attack against the `ResNetDetector` defense.\n","This defense takes the standard `ResNet` classifier from before, and adds an additional detector network.\n","\n","The defense can be used for classification, in which case it outputs an array of scores for each of the 1000 classes, for each input:\n","\n","```\n","resnet_det = ResNetDetector(device)\n","resnet_det.get_logits(x) -> [N, 1000]\n","```\n","\n","To obtain a detector, we trained a *binary* classifier that takes in an input and outputs binary logits for the task of distinguishing clean images (class 0) from adversarially perturbed ones (class 1):\n","\n","```\n","resnet_det = ResNetDetector(device)\n","resnet_det.get_detection_logits(x) -> [N, 2]\n","```\n","\n","*(the classifier and detector actually share most of their implementation.\n","The original ResNet classifier is of the form `g(f(x))` where `f` is a <u>feature extractor</u> that maps inputs to feature vectors, and `g` is a <u>linear layer</u> that maps a feature vector to a vector of 1000 class scores.\n","The detector takes as input the same feature vector `f(x)`, and applies a different linear layer `g_det` that maps the features to a vector of 2 class scores.\n","See `infoseclab.defenses.defense_detector.ResNetDetector` for details).*\n","\n","Note: You are allowed to use the `ResNetDetector` module in your attack, but you are not allowed to modify it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"elapsed":100791,"status":"error","timestamp":1699132219206,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"W7V6HnP5b4LR","outputId":"eea857ba-3555-434e-c101-1bca74cd7cc8"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 71%|███████   | 24/34 [01:40<00:41,  4.18s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-7e84f3965e01>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mdefense_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefenses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNetDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mpgd_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPGD_Det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefense_det\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mx_adv_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgd_det\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImageNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESULTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x_adv_detect.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_adv_det\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-76ae51b15bfe>\u001b[0m in \u001b[0;36mattack_all\u001b[0;34m(self, images, labels, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0madversarial\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \"\"\"\n\u001b[0;32m---> 60\u001b[0;31m         return utils.batched_func(self.attack_batch, inputs=(images, labels),\n\u001b[0m\u001b[1;32m     61\u001b[0m                                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                   device=self.clf.device)\n","\u001b[0;32m/content/infoseclab_23/infoseclab/utils.py\u001b[0m in \u001b[0;36mbatched_func\u001b[0;34m(func, inputs, device, batch_size, disable_tqdm, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-7e84f3965e01>\u001b[0m in \u001b[0;36mattack_batch\u001b[0;34m(self, x, y_targets)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m           \u001b[0moutputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_detection_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/infoseclab_23/infoseclab/defenses/defense_detector.py\u001b[0m in \u001b[0;36mget_logits\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \"\"\"\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_detection_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/infoseclab_23/infoseclab/defenses/defense.py\u001b[0m in \u001b[0;36mget_logits\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["class PGD_Det(PGD):\n","    \"\"\"\n","    A targeted PGD attack that also tries to evade detection.\n","    \"\"\"\n","\n","    def __init__(self, epsilon, clf):\n","        \"\"\"\n","        :param epsilon: the maximum perturbation allowed\n","        :param clf: the classifier to attack\n","        \"\"\"\n","        super().__init__(epsilon, clf)\n","\n","    def attack_batch(self, x, y_targets):\n","        \"\"\"\n","        Attack a batch of images with targeted PGD while also evading detection.\n","        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","        :param y_targets: the target labels of size (batch_size,)\n","        :return: the adversarial images of size (batch_size, 3, 224, 224)\n","        \"\"\"\n","\n","        loss = torch.nn.CrossEntropyLoss()\n","        x_orig = x\n","        alpha = 0.2\n","        target = torch.tensor([1.0, 0.0]).to(device)\n","        target = target.repeat(x.size(dim=0), 1)\n","\n","        for i in range(30):\n","          x.requires_grad = True\n","          outputs = self.clf.get_logits(x)\n","          outputs2 = self.clf.get_detection_logits(x)\n","\n","          cost = loss(outputs, y_targets) + loss(outputs2, target)\n","\n","          cost.backward()\n","\n","          with torch.no_grad():\n","            x_adv = x - alpha * x.grad.sign()\n","            x = self.project(x_adv, x_orig)\n","        return x\n","\n","defense_det = defenses.ResNetDetector(device)\n","pgd_det = PGD_Det(epsilon=EPSILON, clf=defense_det)\n","x_adv_det = pgd_det.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n","\n","utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), x_adv_det)\n","evaluation.eval_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), device=device);"]},{"cell_type":"markdown","metadata":{"id":"eMlKoqzTcLNV"},"source":["## 2.2&nbsp;Evading a Random Forest Detector\n","\n","You will now try to attack a second detector defense.\n","As with the previous defense, we first classify the input in a standard way using the ResNet50 model.\n","\n","But this time, the detector is an opaque \"Random Forest\" model that takes as\n","input the features from the resnet model and outputs a decision.\n","This is a discrete model (a Random Forest is a collection of decision trees) that cannot be easily differentiated. So you'll need\n","a new strategy!\n","\n","**For this defense, your attack can be untargeted. That is, it suffices that the adversarial examples are classified into <i>any</i> incorrect class, as long as the detector doesn't flag the sample.**"]},{"cell_type":"code","source":["class PGD_Det_RF(PGD):\n","    \"\"\"\n","    A targeted PGD attack that also tries to evade detection with a random forest.\n","    \"\"\"\n","\n","    def __init__(self, epsilon, clf):\n","        \"\"\"\n","        :param epsilon: the maximum perturbation allowed\n","        :param clf: the classifier to attack\n","        \"\"\"\n","        super().__init__(epsilon, clf)\n","\n","    def attack_batch(self, x, y):\n","        \"\"\"\n","        Attack a batch of images with targeted PGD while also evading detection.\n","        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","        :param y: the labels of size (batch_size,)\n","        :return: the adversarial images of size (batch_size, 3, 224, 224)\n","        \"\"\"\n","\n","        loss = torch.nn.CrossEntropyLoss()\n","        x_orig = x.clone()\n","\n","        alpha = 0.05\n","        # dist_toll = 1e-2\n","\n","        # while True:\n","        x = x_orig.clone()\n","        # rand_target = torch.randint(1, 10, y.size()).to(device) + y\n","        rand_target = y.clone()\n","        # print(y)\n","\n","        batch_size = y.size(dim=0)\n","        for i in range(batch_size):\n","          rand_target[i] += 1\n","\n","        # print(rand_target)\n","\n","        for i in range(50):\n","          x.requires_grad = True\n","          real_pred = self.clf.get_logits(x_orig)\n","          adv_pred = self.clf.get_logits(x)\n","          # dist = torch.norm(adv_pred - real_pred, p=2) - dist_toll\n","          dist = torch.norm(adv_pred - real_pred, p=2)\n","          ce = loss(adv_pred, rand_target)\n","\n","          cost = 3 * ce + 0.01 * dist\n","          cost.backward()\n","\n","          with torch.no_grad():\n","            x = x - alpha * x.grad.sign()\n","            x = self.project(x, x_orig)\n","          # if torch.equal(self.clf.classify(x), rand_target):\n","          #   break\n","        return x\n","\n","defense_det_rf = defenses.RFDetector(device)\n","pgd_det_rf = PGD_Det_RF(epsilon=EPSILON, clf=defense_det_rf)\n","x_adv_det_rf = pgd_det_rf.attack_all(ImageNet.clean_images, ImageNet.labels, batch_size=6)\n","\n","utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_detect_rf.npy\"), x_adv_det_rf)\n","evaluation.eval_rf_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_detect_rf.npy\"), device=device);\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYJ4ciLvGfHT","executionInfo":{"status":"ok","timestamp":1699266408411,"user_tz":-60,"elapsed":227921,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"}},"outputId":"7ace169a-ee91-4eb9-d3b7-f7bea3ef3a31"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.1.3 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.1.3 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","100%|██████████| 34/34 [03:42<00:00,  6.55s/it]\n","/content/infoseclab_23/infoseclab/utils.py:47: UserWarning: Images are not the same after saving and loading.\n","  warnings.warn(\"Images are not the same after saving and loading.\")\n"]},{"output_type":"stream","name":"stdout","text":["=== Evaluating untargeted PGD with Random Forest Detector ===\n","\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n","\u001b[32m\tadv accuracy: 1.0% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tclean examples detected: 0.0% (target: ≤5.0%)\u001b[0m\n","\u001b[32m\tadv examples detected: 0.0% (target: ≤1.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FBBIk2YcAdg"},"outputs":[],"source":["# class PGD_Det_RF(PGD):\n","#     \"\"\"\n","#     A targeted PGD attack that also tries to evade detection with a random forest.\n","#     \"\"\"\n","\n","#     def __init__(self, epsilon, clf):\n","#         \"\"\"\n","#         :param epsilon: the maximum perturbation allowed\n","#         :param clf: the classifier to attack\n","#         \"\"\"\n","#         super().__init__(epsilon, clf)\n","\n","#     def attack_batch(self, x, y):\n","#         \"\"\"\n","#         Attack a batch of images with targeted PGD while also evading detection.\n","#         :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","#         :param y: the labels of size (batch_size,)\n","#         :return: the adversarial images of size (batch_size, 3, 224, 224)\n","#         \"\"\"\n","#         loss = torch.nn.CrossEntropyLoss()\n","#         x_orig = x.clone()\n","#         x_adv = x.clone()\n","#         alpha = 2\n","\n","#         rand_vec = torch.randint(0, 999, y.size()).to(device)\n","\n","#         counter = 0\n","#         batch_size = y.size(dim=0)\n","#         while counter < batch_size:\n","#           for i in range(20):\n","#             x.requires_grad = True\n","#             # real_pred = self.clf.get_logits(x_orig)\n","#             adv_pred = self.clf.get_logits(x)\n","\n","#             cost = loss(adv_pred, rand_vec)\n","#             cost.backward()\n","\n","#             with torch.no_grad():\n","#               x = x - alpha * x.grad.sign()\n","#               x = self.project(x_adv, x_orig)\n","\n","#             # classify adv example\n","#             pred = self.clf.detect(x)\n","#           # print(pred)\n","#           # print(rand_vec)\n","#             # check the prediction\n","#           if pred[counter] == 0:\n","#             x_adv[counter] = x[counter].clone() # save the correct one\n","#             counter += 1\n","#           else:\n","#             rand_vec[counter] = torch.randint(0, 999, (1,))[0].to(device)\n","\n","#         return x_adv\n","\n","\n","# defense_det_rf = defenses.RFDetector(device)\n","# pgd_det_rf = PGD_Det_RF(epsilon=EPSILON, clf=defense_det_rf)\n","# x_adv_det_rf = pgd_det_rf.attack_all(ImageNet.clean_images, ImageNet.labels, batch_size=6)\n","\n","# utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_detect_rf.npy\"), x_adv_det_rf)\n","# evaluation.eval_rf_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_detect_rf.npy\"), device=device);\n"]},{"cell_type":"markdown","metadata":{"id":"EdorMrPt-Vpf"},"source":["#3.&nbsp; Preprocessing Defenses\n","\n","We are now going to look at two defenses against adversarial examples that aim to resist noise by *pre-processing* the input before classifying it.\n","\n","<ul>\n","  <li> 3.1. Blurring </li>\n","  <li> 3.2. Random cropping and noising </li>\n","  <li> 3.3. Input discretization </li>\n","<ul>"]},{"cell_type":"markdown","metadata":{"id":"3y_q-ox3c_qb"},"source":["##3.1&nbsp; Blurring\n","\n","A natural defense idea is to try and remove the noise from adversarial images. A simple way of trying to do that is to add a blurring filter.\n","\n","The `ResNetBlur` defense implements this. Your goal is to create a targeted PGD attack that will defeat Blurring.\n","You **don't** need to ensure that the attack stays undetected.\n","\n","The challenge you'll encounter is that the Blurring filter we use is not automatically differentiable. You'll likely need to find a way around that!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194032,"status":"ok","timestamp":1699112503655,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"GlbbYrsIgnsc","outputId":"86ade1bd-17db-48ed-c61f-2f93c89c88c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 34/34 [02:58<00:00,  5.24s/it]\n"]},{"name":"stdout","output_type":"stream","text":["=== Evaluating targeted PGD on blurred defense ===\n","\u001b[32m\tclean accuracy: 92.5%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.5% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 99.5% (target: ≥98.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n"]}],"source":["class PGD_Blur(PGD):\n","    \"\"\"\n","    A targeted PGD attack that tries to resist Blurring.\n","    \"\"\"\n","\n","    def __init__(self, epsilon, clf):\n","        \"\"\"\n","        :param epsilon: the maximum perturbation allowed\n","        :param clf: the classifier to attack\n","        \"\"\"\n","        super().__init__(epsilon, clf)\n","\n","\n","    def attack_batch(self, x, y_targets):\n","        \"\"\"\n","        Attack a batch of images with targeted PGD while also resisting JPEG compression.\n","        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","        :param y_targets: the target labels of size (batch_size,)\n","        :return: the adversarial images of size (batch_size, 3, 224, 224)\n","        \"\"\"\n","        resnet = defenses.ResNet(device)\n","\n","        loss = torch.nn.CrossEntropyLoss()\n","        x_orig = x.clone()\n","        x_adv = x.clone()\n","        alpha = 0.2\n","\n","        for i in range(30):\n","          gx = defenses.defense_blur.blur_images(x_adv)\n","          gx.requires_grad = True\n","          outputs = resnet.get_logits(gx)\n","\n","          cost = loss(outputs, y_targets)\n","          cost.backward()\n","\n","          with torch.no_grad():\n","            x_adv = x_adv - alpha * gx.grad.sign()\n","            x_adv = self.project(x_adv, x_orig)\n","\n","        return x_adv\n","\n","\n","defense_blur = defenses.ResNetBlur(device)\n","pgd_blur = PGD_Blur(epsilon=EPSILON, clf=defense_blur)\n","x_adv_blur = pgd_blur.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n","\n","utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_blur.npy\"), x_adv_blur)\n","evaluation.eval_blur_attack(os.path.join(RESULTS_PATH, \"x_adv_blur.npy\"), device);"]},{"cell_type":"markdown","metadata":{"id":"ag6TwSfy_Y8Z"},"source":["##3.2&nbsp; Randomized cropping and noising\n","\n","Another natural defense idea is to try and *randomize* the model's behavior to make it harder to create adversarial examples.\n","\n","The ResNetRandom defense implements this, by randomly cropping and noising input images before classifying them.\n","\n","Your goal is to create a targeted PGD attack that will defeat randomized pre-processing.\n","In this part, you **don't** need to ensure that the attack stays undetected.\n","\n","**Note that since this defense is randomized, the evaluation results might vary slightly from one run to the next. To make sure that your attack passes our final evaluation, try to create an attack that has a few % of slack compared to the evaluation targets (e.g., if we target an adversarial accuracy below 5%, aim to ensure that your attack reaches ~3% or lower)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sH64CeqCAzFC","executionInfo":{"status":"ok","timestamp":1699121334957,"user_tz":-60,"elapsed":446366,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"}},"outputId":"8bff0858-1702-4a0d-e045-59096d805423"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 34/34 [07:22<00:00, 13.00s/it]\n"]},{"output_type":"stream","name":"stdout","text":["=== Evaluating targeted PGD on randomized defense ===\n","\u001b[32m\tclean accuracy: 93.5%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.5% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 99.5% (target: ≥98.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n"]}],"source":["class PGD_Random(PGD):\n","    \"\"\"\n","    A PGD attack that also tries to resist random preprocessing.\n","    \"\"\"\n","\n","    def __init__(self, epsilon, clf):\n","        super().__init__(epsilon, clf)\n","\n","    def attack_batch(self, x, y_targets):\n","        \"\"\"\n","        Attack a batch of images with targeted PGD while also evading random preprocessing.\n","        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","        :param y_targets: the target labels of size (batch_size,)\n","        :return: the adversarial images of size (batch_size, 3, 224, 224)\n","        \"\"\"\n","        loss = torch.nn.CrossEntropyLoss()\n","        x_orig = x.clone()\n","        alpha = 2\n","        it = 10\n","        sum_grad = 0.0\n","\n","        for i in range(20):\n","          for j in range(it):\n","            x.requires_grad = True\n","            outputs = self.clf.get_logits(x)\n","\n","            cost = loss(outputs, y_targets)\n","            cost.backward()\n","            grad = x.grad.detach()\n","            sum_grad += grad\n","          exp_grad = sum_grad / it\n","\n","          with torch.no_grad():\n","            x = x - alpha * exp_grad.sign()\n","            x = self.project(x, x_orig)\n","\n","        return x\n","\n","\n","defense_random = defenses.ResNetRandom(device)\n","pgd_random = PGD_Random(epsilon=EPSILON, clf=defense_random)\n","x_adv_random = pgd_random.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n","\n","utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_random.npy\"), x_adv_random)\n","evaluation.eval_random_attack(os.path.join(RESULTS_PATH, \"x_adv_random.npy\"), device);"]},{"cell_type":"markdown","metadata":{"id":"8IqqopaZfTdS"},"source":["##3.3&nbsp; Discretized inputs\n","\n","This final defense does something a bit crazy to make it hard for you to compute gradients (a lot of proposed defenses against adversarial examples used to do this, but none of them work...)\n","\n","This defense discretizes the model's inputs as follows: each pixel (in the range [0, 1]) is mapped to an array of 20 binary \"buckets\", where the i-th bucket is set if the input pixel is greater than i/20. So for example we encode `0.0` as `[0 0 0 0 0 ... 0]`, `0.1` as `[1 1 0 0 ... 0]` and `1.0` as `[1 1 1 1 1 ... 1]`.\n","\n","See the `ResNetDiscrete` defense for the implementation of this encoding.\n","\n","The encoded inputs are then fed into a ResNet-style model (which was modified to take in inputs with `3*20` pixel channels instead of `3`).\n","\n","Your goal is to create targeted adversarial examples that break this defense."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"-CHBUCELfS16","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699218480995,"user_tz":-60,"elapsed":67607,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"}},"outputId":"7fc497eb-6724-4502-b838-ad52ef03b23a"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 34/34 [01:03<00:00,  1.88s/it]\n","/content/infoseclab_23/infoseclab/utils.py:47: UserWarning: Images are not the same after saving and loading.\n","  warnings.warn(\"Images are not the same after saving and loading.\")\n"]},{"output_type":"stream","name":"stdout","text":["=== Evaluating targeted PGD on discretized defense ===\n","\u001b[32m\tclean accuracy: 80.5%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.0% (target: ≤ 3.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 100.0% (target: ≥96.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n"]}],"source":["class PGD_Discrete(PGD):\n","    \"\"\"\n","    A PGD attack that also tries to resist input discretization.\n","    \"\"\"\n","\n","    def __init__(self, epsilon, clf):\n","        super().__init__(epsilon, clf)\n","\n","    def encode(self, x):\n","\n","      # put the channels last\n","      x = x.permute(0, 2, 3, 1)\n","      shape = list(x.shape)\n","\n","      # add a dummy dimension\n","      x = x.unsqueeze(-1)\n","\n","      # discretize into 20 bins\n","      thresholds = torch.from_numpy(np.arange(0, 1, .05)).to(x.device).float()\n","      # gt_threshold = x > thresholds\n","\n","      # use sigmoid to simulate the discretized behaviour, but perserving the differetiality\n","      x = torch.sigmoid(x - thresholds)\n","\n","      # reshape to original shape with 20x channels\n","      shape[-1] *= thresholds.shape[0]\n","      x = x.reshape(shape)\n","      x = x.permute(0, 3, 1, 2)\n","\n","      assert x.shape[1:] == (60, 224, 224)\n","      return x\n","\n","    def attack_batch(self, x, y_targets):\n","        \"\"\"\n","        Attack a batch of images with targeted PGD while also evading random preprocessing.\n","        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n","        :param y_targets: the target labels of size (batch_size,)\n","        :return: the adversarial images of size (batch_size, 3, 224, 224)\n","        \"\"\"\n","        loss = torch.nn.CrossEntropyLoss()\n","        x_orig = x.clone()\n","        x_adv = x.clone()\n","        alpha = 0.2\n","\n","        for i in range(30):\n","          x_enc = self.clf.encode(x / 255.0)\n","          x_enc.requires_grad = True\n","          # outputs = self.clf.get_logits(x)\n","          outputs = self.clf.model(x_enc)\n","          cost = loss(outputs, y_targets)\n","          cost.backward()\n","\n","          # compute the gradient \"by hand\"\n","          gradient = x_enc.grad\n","          res = torch.autograd.functional.vjp(self.encode, x / 255.0, gradient)[1]\n","\n","          with torch.no_grad():\n","            x = x - alpha * res.sign()\n","            x = self.project(x, x_orig)\n","\n","        return x\n","\n","\n","defense = defenses.ResNetDiscrete(device)\n","pgd_discrete = PGD_Discrete(epsilon=EPSILON, clf=defense)\n","x_adv_discrete = pgd_discrete.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n","\n","utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_discrete.npy\"), x_adv_discrete)\n","evaluation.eval_discrete_attack(os.path.join(RESULTS_PATH, \"x_adv_discrete.npy\"), device);"]},{"cell_type":"markdown","metadata":{"id":"9_Cmw-Qj8mLe"},"source":["# Create submission file (**upload `results1.zip` to moodle**)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"197yEKu1J_-l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699266952903,"user_tz":-60,"elapsed":286,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"}},"outputId":"26d7d0cc-7ac1-4b16-bacb-82ecc84d9e9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive is current\n"]}],"source":["!zip -j -FSr \"{RESULTS_PATH}/results1.zip\" {RESULTS_PATH}"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32308,"status":"ok","timestamp":1699266987582,"user":{"displayName":"Jacomo Jiang (Jacomo)","userId":"08104917559812693081"},"user_tz":-60},"id":"yW3j3t9y9ZVO","outputId":"a7d9ec38-e8fb-4748-8936-be32d73f5541"},"outputs":[{"output_type":"stream","name":"stdout","text":["Zip file is valid\n","=== Evaluating targeted PGD ===\n","\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.0% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 100.0% (target: ≥98.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n","=== Evaluating targeted PGD with Neural Network Detector ===\n","\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.0% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 100.0% (target: ≥98.0%)\u001b[0m\n","\u001b[32m\tclean examples detected: 2.5% (target: ≤5.0%)\u001b[0m\n","\u001b[32m\tadv examples detected: 0.0% (target: ≤1.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n","=== Evaluating untargeted PGD with Random Forest Detector ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.1.3 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.1.3 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n","\u001b[32m\tadv accuracy: 1.0% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tclean examples detected: 0.0% (target: ≤5.0%)\u001b[0m\n","\u001b[32m\tadv examples detected: 0.0% (target: ≤1.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n","=== Evaluating targeted PGD on blurred defense ===\n","\u001b[32m\tclean accuracy: 92.5%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.5% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 99.5% (target: ≥98.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n","=== Evaluating targeted PGD on randomized defense ===\n","\u001b[32m\tclean accuracy: 90.5%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.5% (target: ≤ 2.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 100.0% (target: ≥98.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n","=== Evaluating targeted PGD on discretized defense ===\n","\u001b[32m\tclean accuracy: 80.5%\u001b[0m\n","\u001b[32m\tadv accuracy: 0.0% (target: ≤ 3.0%)\u001b[0m\n","\u001b[32m\tadv target accuracy: 100.0% (target: ≥96.0%)\u001b[0m\n","\u001b[32mSUCCESS\u001b[0m\n"]}],"source":["from infoseclab.evaluation import eval_targeted_pgd, eval_detector_attack, eval_rf_detector_attack, eval_blur_attack, eval_random_attack, eval_discrete_attack\n","from infoseclab.submission import validate_zip1\n","\n","assert validate_zip1(f\"{RESULTS_PATH}/results1.zip\")\n","\n","with ZipFile(f\"{RESULTS_PATH}/results1.zip\", 'r') as zip:\n","    _ = eval_targeted_pgd(path=zip.open(\"x_adv_targeted.npy\"), device=device)\n","    _ = eval_detector_attack(path=zip.open(\"x_adv_detect.npy\"), device=device)\n","    _ = eval_rf_detector_attack(path=zip.open(\"x_adv_detect_rf.npy\"), device=device)\n","    _ = eval_blur_attack(path=zip.open(\"x_adv_blur.npy\"), device=device)\n","    _ = eval_random_attack(path=zip.open(\"x_adv_random.npy\"), device=device)\n","    _ = eval_discrete_attack(path=zip.open(\"x_adv_discrete.npy\"), device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1JLHeI60J1MhFD6M6yvrVOYYx3pkOQ43S","timestamp":1698135701391},{"file_id":"1tkEZthhMM_UH8MxRmsbFTJt0hmCSDkod","timestamp":1669122254837}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}